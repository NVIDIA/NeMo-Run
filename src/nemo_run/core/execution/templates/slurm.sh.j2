{%- import "ft_launcher.j2" as fault_tolerance -%}
#!/bin/bash
#
# Generated by NeMo Run
# Run with: {{sbatch_command}}
#

# Parameters
{%- for sbatch_flag in sbatch_flags %}
{{sbatch_flag}}
{%- endfor %}

set -evx

export PYTHONUNBUFFERED=1
export SLURM_UNBUFFEREDIO=1
export TORCHX_MAX_RETRIES={{max_retries}}
{%- for env_var in env_vars %}
{{env_var}}
{%- endfor %}

set +e

# setup

nodes=( $( scontrol show hostnames $SLURM_JOB_NODELIST ) )
nodes_array=($nodes)
head_node=${nodes_array[0]}
{{head_node_ip_var}}=$(srun --nodes=1 --ntasks=1 -w "$head_node" hostname --ip-address)
{% if heterogeneous %}
{% for i in range(srun_commands|length) %}
{{het_group_host_var}}_{{i}}=$(scontrol show hostnames=$SLURM_JOB_NODELIST_HET_GROUP_{{i}} | head -n1)
{%- endfor %}
{% endif %}

{%- if setup_lines %}
{{setup_lines}}
{%- endif %}

{%- if ft_enabled %}
{{ fault_tolerance.ft_launcher_setup(fault_tol_cfg_path, fault_tol_finished_flag_file, fault_tol_job_results_file) }}
{%- endif %}

{%- if memory_measure %}
srun --ntasks=1 --ntasks-per-node=1 --output {{memory_measure}} --wait=60 --kill-on-bad-exit=1 --overlap nvidia-smi
{%- endif %}

{% for srun_command in srun_commands %}
# Command {{loop.index}}

{%- if loop.index <= group_env_vars|length %}
{% for env_var in group_env_vars[loop.index - 1] %}
{{env_var}}
{% endfor %}
{%- endif %}

{{srun_command}}
{% endfor %}

{%- if monitor_group_job %}

# The code below monitors all SLURM jobs to ensure any failure forces them all to stop
# (otherwise some jobs may remain pending until they reach the cluster time limit).
all_done=false
while ! $all_done; do
    all_done=true
    for pid in "${pids[@]}"; do
        if ps -p "$pid" > /dev/null; then
            # Process is still running.
            all_done=false
        else
            # Process is no longer running => check its exit status.
            wait "$pid"
            exitcode=$?
            echo "Process $pid exited with code $exit_code at $(date '+%Y-%m-%d %H:%M:%S')"
            # Wait a bit (to get a clean stack trace in case there is one being generated), then kill the
            # remaining processes if needed.
            sleep {{monitor_group_job_wait_time}}
            for other_pid in "${pids[@]}"; do
                if ps -p "$other_pid" > /dev/null; then
                    echo "Killing process $other_pid"
                    kill -9 "$other_pid"
                fi
            done
            break 2
        fi
    done

    # Sleep for a while before checking again.
    sleep {{monitor_group_job_wait_time}}
done
{% elif run_as_group %}
wait

exitcode=$?
{% else %}
exitcode=$?
{% endif %}
set -e

echo "job exited with code $exitcode"

{%- if ft_enabled %}
{{ fault_tolerance.ft_launcher_teardown() }}
{%- else %}
if [ $exitcode -ne 0 ]; then
    if [ "$TORCHX_MAX_RETRIES" -gt "${SLURM_RESTART_COUNT:-0}" ]; then
        scontrol requeue "$SLURM_JOB_ID"
    fi
    exit $exitcode
fi
{%- endif %}
