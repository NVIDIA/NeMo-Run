#!/bin/bash
#
# Generated by NeMo Run
#

# Parameters
{%- for sbatch_flag in sbatch_flags %}
{{sbatch_flag}}
{%- endfor %}

set -eoux pipefail

########################################################
# User defined variables
########################################################
export PYTHONUNBUFFERED=1
export SLURM_UNBUFFEREDIO=1

{%- for env_var in env_vars %}
{{env_var}}
{%- endfor %}

# Ports for all nodes (should be odd numbers since we place head/worker[0] on the same node) so all workers get the odd ports, but the head will get +1 the ports
NODE_MANAGER_PORT=${NODE_MANAGER_PORT:-53001}
OBJECT_MANAGER_PORT=${OBJECT_MANAGER_PORT:-53003}
RUNTIME_ENV_AGENT_PORT=${RUNTIME_ENV_AGENT_PORT:-53005}
DASHBOARD_AGENT_GRPC_PORT=${DASHBOARD_AGENT_GRPC_PORT:-53007}
METRICS_EXPORT_PORT=${METRICS_EXPORT_PORT:-53009}

# Ports for the head node
PORT=${PORT:-6379}
RAY_CLIENT_SERVER_PORT=${RAY_CLIENT_SERVER_PORT:-10001}
#REDIT_SHARD_PORTS=${REDIT_SHARD_PORTS:-"random"} ??
DASHBOARD_GRPC_PORT=${DASHBOARD_GRPC_PORT:-52367}
DASHBOARD_PORT=${DASHBOARD_PORT:-8265}  # Also used by debugger
DASHBOARD_AGENT_LISTEN_PORT=${DASHBOARD_AGENT_LISTEN_PORT:-52365}

# On our clusters, the largest port range on an idle worker appeared between 52369-64607
# (not including the other ports set by this script). So this range is chosen to be
# somewhere in the middle
MIN_WORKER_PORT=${MIN_WORKER_PORT:-54001}
MAX_WORKER_PORT=${MAX_WORKER_PORT:-54257}

# Directory setup
export CLUSTER_DIR={{ cluster_dir }}
mkdir -p $CLUSTER_DIR

JOB_IDS_FILE="$CLUSTER_DIR/job_ids.json"
if [[ -f "$JOB_IDS_FILE" ]]; then
  tmp="$(mktemp)"
  jq --arg id "$SLURM_JOB_ID" '. + [$id]' "$JOB_IDS_FILE" > "$tmp" && mv "$tmp" "$JOB_IDS_FILE"
else
  touch "$JOB_IDS_FILE"
  echo "[\"$SLURM_JOB_ID\"]" > "$JOB_IDS_FILE"
fi

mkdir -p $CLUSTER_DIR/scripts

export LOG_DIR={{ log_dir }}
mkdir -p $LOG_DIR

# Clean up any previous run files
rm -f $LOG_DIR/STARTED_RAY_HEAD
rm -f $LOG_DIR/ENDED

# Defaults to placing uv cache inside the CLUSTER_DIR
# This directory is mounted into the container at /home/ray/.cache/uv so it is shared between the head and worker nodes
# UV_CACHE_DIR={{ uv_cache_dir }}
# mkdir -p $UV_CACHE_DIR
########################################################

# Number of GPUs per node
gpus_per_node=8

num_retries={{ num_retries }}

# Getting the node names and IP addresses in the SLURM allocation
nodes=$(scontrol show hostnames "$SLURM_JOB_NODELIST")
nodes_array=($nodes)
ip_addresses_array=()

for node in $nodes; do
    ip_address=$(host $node | awk '/has address/ { print $4 }')
    # Add the IP address to the array
    ip_addresses_array+=("$ip_address")
done

head_node=${nodes_array[0]}
head_node_ip=${ip_addresses_array[0]}

ip_head=$head_node_ip:$PORT

{%- if setup_lines %}
{{setup_lines}}
{%- endif %}

########################################################
# Ray cluster setup
########################################################
# First we start the head of the ray cluster on one of the physical nodes
# Set GPU/CPU resources to 0 to avoid scheduling on the head node

head_cmd=$(cat <<EOF
# Touch a file to indicate that the head node has started
# Overlapping srun commands will check this file to determine if we can overlap a container command
touch $LOG_DIR/STARTED_RAY_HEAD
env

{%- if pre_ray_start_commands %}
{{ pre_ray_start_commands }}
{%- endif %}

exit-dramatically() {
    # Use SIGTERM to forcefully terminate the srun process
    pkill -P $$ || true
    kill -TERM 0 || true
    # As a last resort, exit with a non-zero code
    exit 1
}
export -f exit-dramatically

# Background process to check for ENDED file
monitor-sidecar() {
  set +x
  while true; do
    sleep 60
    if [[ -f "$LOG_DIR/ENDED" ]]; then
      echo "Detected ENDED file, terminating..."
      exit-dramatically
    fi
  done
}
monitor-sidecar &

cat <<EOFINNER | tee /launch-head.sh
ray start --head \
    --disable-usage-stats \
    --temp-dir=/ray-cluster \
    --resources="{\"worker_units\": $gpus_per_node, \"slurm_managed_ray_cluster\": 1}" \
    --node-ip-address="$head_node_ip" \
    --port=${PORT} \
    --ray-client-server-port=${RAY_CLIENT_SERVER_PORT} \
    --dashboard-grpc-port=${DASHBOARD_GRPC_PORT} \
    --dashboard-port=${DASHBOARD_PORT} \
    \
    --node-manager-port=${NODE_MANAGER_PORT} \
    --object-manager-port=${OBJECT_MANAGER_PORT} \
    --runtime-env-agent-port=${RUNTIME_ENV_AGENT_PORT} \
    --dashboard-agent-grpc-port=${DASHBOARD_AGENT_GRPC_PORT} \
    --dashboard-agent-listen-port=${DASHBOARD_AGENT_LISTEN_PORT} \
    --metrics-export-port=${METRICS_EXPORT_PORT} \
    \
    --block
EOFINNER
chmod +x /launch-head.sh

count=0
while [[ \$count -lt $num_retries ]]; do
  bash /launch-head.sh
  count=\$((count+1))
  echo "Head node failed \$count/$num_retries times, restarting in 5 seconds..."
  sleep 5
done
touch $LOG_DIR/ENDED
exit 1
EOF
)
srun {{ common_srun_args }} --container-name=ray-head --nodes=1 --ntasks=1 -w "$head_node" -o $LOG_DIR/ray-head.log bash -x -c "$head_cmd" &

# Wait for the head node container to start and for Ray to be ready
while ! (srun --overlap --nodes=1 --ntasks=1 -w $head_node test -f $LOG_DIR/STARTED_RAY_HEAD && srun --overlap --container-name=ray-head --nodes=1 --ntasks=1 -w $head_node ray status --address $ip_head 2>/dev/null); do
  echo "[INFO][$(date)] Waiting for Ray head node container to start and be ready..."
  sleep 2
done

NUM_ACTORS=$((gpus_per_node * SLURM_JOB_NUM_NODES))

# Start Ray worker nodes
# We want 1 Ray worker node per physical node
# Worker nodes are started with ray start but without the --head flag
for ((i = 1; i < SLURM_JOB_NUM_NODES; i++)); do
  node_i=${nodes_array[$i]}

  worker_cmd=$(cat <<EOF
env

{%- if pre_ray_start_commands %}
{{ pre_ray_start_commands }}
{%- endif %}

exit-dramatically() {
    # Use SIGTERM to forcefully terminate the srun process
    pkill -P $$ || true
    kill -TERM 0 || true
    # As a last resort, exit with a non-zero code
    exit 1
}

# Background process to check for ENDED file
monitor-sidecar() {
  set +x
  while true; do
    sleep 60
    if [[ -f "$LOG_DIR/ENDED" ]]; then
      echo "Detected ENDED file, terminating..."
      exit-dramatically
    fi
  done
}
monitor-sidecar &

cat <<EOFINNER | tee /launch-worker.sh
sleep 5
ray start --address "$ip_head" \
          --disable-usage-stats \
          --resources="{\"worker_units\": $gpus_per_node, \"slurm_managed_ray_cluster\": 1}" \
          --min-worker-port=${MIN_WORKER_PORT} \
          --max-worker-port=${MAX_WORKER_PORT} \
          \
          --node-manager-port=${NODE_MANAGER_PORT} \
          --object-manager-port=${OBJECT_MANAGER_PORT} \
          --runtime-env-agent-port=${RUNTIME_ENV_AGENT_PORT} \
          --dashboard-agent-grpc-port=${DASHBOARD_AGENT_GRPC_PORT} \
          --dashboard-agent-listen-port=${DASHBOARD_AGENT_LISTEN_PORT} \
          --metrics-export-port=${METRICS_EXPORT_PORT} \
          \
          --block
EOFINNER

count=0
while [[ \$count -lt $num_retries ]]; do
  bash /launch-worker.sh
  count=\$((count+1))
  echo "Worker failed \$count/$num_retries times, restarting in 5 seconds..."
  sleep 5
done
touch $LOG_DIR/ENDED
exit 1
EOF
)
  if [[ $i -eq 0 ]]; then
    OVERLAP_HEAD_AND_WORKER_ARG="--overlap"
  fi
  srun {{ common_srun_args }} ${OVERLAP_HEAD_AND_WORKER_ARG:-} --container-name=ray-worker-$i --exact --nodes=1 --ntasks=1 --cpus-per-task=$((16 * gpus_per_node)) -w "$node_i" -o $LOG_DIR/ray-worker-$i.log bash -x -c "$worker_cmd" &
  sleep 3
done

# At this stage the Ray cluster bringup has started on the physical nodes in the allocation
# Before we launch a job on this cluster we need to make sure that the bringup is complete
# We do so by querying the number of worker_units in the ray cluster and asserting = NUM_ACTORS
extract_worker_units() {
  status_output=$(srun --overlap --container-name=ray-head --nodes=1 --ntasks=1 -w "$head_node" ray status --address $ip_head)
  if echo "$status_output" | grep -q "worker_units"; then
    worker_units=$(echo "$status_output" | grep "worker_units" | awk -F'[/. ]' '{print $4}')
    echo $worker_units
  else
    echo 0
  fi
}

# Poll to make sure that all Ray worker nodes have connected to the head.
# All workers have connected when number of GPUs in ray cluster
# is equal to NUM_ACTORS. We use the utility function above
# to check how many GPUs have come online in the ray cluster
while true; do
  worker_units=$(extract_worker_units)
  echo "[INFO] Number of actors online: $worker_units/$NUM_ACTORS"
  if [ "$worker_units" -eq "$NUM_ACTORS" ]; then
    break
  fi
  sleep 2
done

echo "All workers connected!"

# Create JSON with Ray cluster information
cat <<EOF >$CLUSTER_DIR/ray_cluster_info.json
{
  "head_ip": "$head_node_ip",
  "dashboard_port": "$DASHBOARD_PORT",
  "port": "$PORT"
}
EOF
# Set up trap to clean up cluster info on job termination
cleanup_cluster_info() {
    echo "[INFO] Cleaning up Ray cluster information"
    rm -f $CLUSTER_DIR/ray_cluster_info.json
}

# Register the cleanup function to run on script exit
trap cleanup_cluster_info EXIT


echo "[INFO] Ray cluster information saved to $CLUSTER_DIR/ray_cluster_info.json"

########################################################

{% if srun_commands %}
# Run extra commands
{% for srun_command in srun_commands %}
{%- if loop.index <= group_env_vars|length %}
{%- for env_var in group_env_vars[loop.index - 1] %}
{{env_var}}
{%- endfor %}
{%- endif %}

{{srun_command}}
{% endfor %}
########################################################
{% endif -%}

# We can now launch a job on this cluster
# We do so by launching a driver process on the physical node that the head node is on
# This driver process is responsible for launching a job on the Ray cluster
CONTAINER_CWD=$(scontrol show job $SLURM_JOB_ID --json | jq -r '.jobs[].current_working_directory')
# Define command to be empty by default
COMMAND="${COMMAND:-{{ command | default('', true) }}}"
COMMAND_WORKDIR={{ command_workdir | default('$CONTAINER_CWD') }}

if [[ -n "$COMMAND" ]]; then
  srun --no-container-mount-home --gpus=0 --overlap --container-name=ray-head --container-workdir=$COMMAND_WORKDIR --nodes=1 --ntasks=1 -w "$head_node" -o $LOG_DIR/ray-job.log bash -c "$COMMAND"
else
  echo "[INFO]: Ray Cluster is idled, run this on the slurm head node to get a shell to the head node:"
  cat <<EOF >$CLUSTER_DIR/scripts/${SLURM_JOB_ID}-attach.sh
# No args launches on the head node
WORKER_NUM=\${1:-}
if [[ -z "\$WORKER_NUM" ]]; then
  # Empty means we are on the head node
  srun --no-container-mount-home --gpus=0 -A $SLURM_JOB_ACCOUNT -p $SLURM_JOB_PARTITION --overlap --container-name=ray-head --container-workdir=$CONTAINER_CWD --nodes=1 --ntasks=1 -w "$head_node" --jobid $SLURM_JOB_ID --pty bash
else
  nodes_array=($nodes)
  srun --no-container-mount-home {%- if gres_specification %}{{gres_specification}}{% endif %} -A $SLURM_JOB_ACCOUNT -p $SLURM_JOB_PARTITION --overlap --container-name=ray-worker-\$WORKER_NUM --container-workdir=$CONTAINER_CWD --nodes=1 --ntasks=1 -w "\${nodes_array[\$WORKER_NUM]}" --jobid $SLURM_JOB_ID --pty bash
fi
EOF
  chmod +x $CLUSTER_DIR/scripts/${SLURM_JOB_ID}-attach.sh
  echo "     bash $CLUSTER_DIR/scripts/${SLURM_JOB_ID}-attach.sh"
  sleep infinity
fi
